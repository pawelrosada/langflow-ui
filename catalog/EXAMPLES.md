# üìñ Przyk≈Çady U≈ºycia AI Catalog

Przyk≈Çady praktycznego wykorzystania katalogu AI w ≈õrodowisku Langflow + Open WebUI.

## üöÄ Przyk≈Çad 1: Import i Konfiguracja Gemini

### Krok 1: Import przep≈Çywu
```bash
# Uruchom ≈õrodowisko
./setup-openwebui.sh

# Otw√≥rz Langflow
# http://localhost:7860
```

### Krok 2: Za≈Çaduj przep≈Çyw Gemini
1. W Langflow kliknij **"+ New Flow"**
2. Wybierz **"Import from JSON"**
3. Za≈Çaduj `catalog/flows/gemini-chat-basic.json`
4. Przep≈Çyw pojawi siƒô z trzema komponentami:
   - **Chat Input** (wiadomo≈õƒá od u≈ºytkownika)
   - **Google Generative AI** (Gemini Pro)
   - **Chat Output** (odpowied≈∫ do u≈ºytkownika)

### Krok 3: Konfiguracja API Key
```json
{
  "google_api_key": "AIza-your-google-api-key-here",
  "model": "gemini-1.5-pro-latest",
  "temperature": 0.1,
  "system_message": "Jeste≈õ pomocnym asystentem AI. Odpowiadaj w jƒôzyku polskim."
}
```

### Krok 4: Test w Playground
```
Wiadomo≈õƒá: "Opowiedz mi o sztucznej inteligencji"
Oczekiwana odpowied≈∫: Szczeg√≥≈Çowa odpowied≈∫ po polsku o AI
```

## üöÄ Przyk≈Çad 2: U≈ºycie Multiple Models

### Szenariusz: R√≥≈ºne AI dla r√≥≈ºnych zada≈Ñ

**GPT-4o dla pisania:**
```bash
@flow:gpt4o-chat-basic Napisz artyku≈Ç o przysz≈Ço≈õci pracy zdalnej
```

**Gemini dla analiz:**
```bash
@flow:gemini-chat-basic Przeanalizuj trendy w bran≈ºy technologicznej
```

**Claude dla kreatywno≈õci:**
```bash
@flow:claude3-chat-basic Wymy≈õl innowacyjny pomys≈Ç na aplikacjƒô mobile
```

## üöÄ Przyk≈Çad 3: Custom Pipeline Integration

### Tworzenie w≈Çasnego pipeline

```python
# custom_ai_pipeline.py
from catalog.pipelines.gemini_chat_pipeline import Pipeline as GeminiPipeline

class CustomMultiModelPipeline:
    def __init__(self):
        self.gemini = GeminiPipeline()
        self.name = "Multi-Model Pipeline"
    
    def pipe(self, user_message: str, model_id: str, messages: list, body: dict):
        # Logika wyboru modelu na podstawie typu zapytania
        if "kod" in user_message.lower() or "program" in user_message.lower():
            return self.gemini.pipe(user_message, model_id, messages, body)
        # ... inne modele
```

## üöÄ Przyk≈Çad 4: Production Setup

### Docker Compose z konfiguracjƒÖ production

```yaml
# docker-compose.prod.yml
services:
  langflow:
    image: langflowai/langflow:latest
    environment:
      - LANGFLOW_DATABASE_URL=postgresql://user:pass@db:5432/langflow
      - LANGFLOW_SECRET_KEY=${LANGFLOW_SECRET_KEY}
    volumes:
      - ./catalog/flows:/app/flows:ro  # Read-only examples
    
  pipelines:
    build: ./catalog/pipelines
    environment:
      - LANGFLOW_BASE_URL=http://langflow:7860
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
```

### Environment Variables
```bash
# .env.production
GEMINI_API_KEY=AIza-your-gemini-key
OPENAI_API_KEY=sk-your-openai-key
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
LANGFLOW_SECRET_KEY=your-secret-key
```

## üöÄ Przyk≈Çad 5: Monitoring i Logging

### Sprawdzenie log√≥w pipeline
```bash
# Logi wszystkich serwis√≥w
docker-compose logs -f

# Logi konkretnego pipeline
docker-compose logs -f pipelines | grep "Gemini"

# Metryki wydajno≈õci
docker-compose exec pipelines python3 -c "
from catalog.pipelines.gemini_chat_pipeline import Pipeline
p = Pipeline()
print(f'Pipeline: {p.name}')
print(f'Config: {p.valves}')
"
```

### Health Check Endpoints
```bash
# Sprawd≈∫ status Langflow
curl http://localhost:7860/health

# Sprawd≈∫ dostƒôpne przep≈Çywy
curl http://localhost:7860/api/v1/flows

# Test pipeline bezpo≈õrednio
curl -X POST http://localhost:9099/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-pipeline",
    "messages": [{"role": "user", "content": "Test wiadomo≈õƒá"}]
  }'
```

## üöÄ Przyk≈Çad 6: Skalowanie i Load Balancing

### Multiple Pipeline Instances
```yaml
services:
  pipelines-gemini:
    build: ./catalog/pipelines
    environment:
      - PIPELINE_TYPE=gemini
      - WORKFLOW_ID=gemini-chat-basic
      
  pipelines-gpt4:
    build: ./catalog/pipelines
    environment:
      - PIPELINE_TYPE=gpt4
      - WORKFLOW_ID=gpt4o-chat-basic
      
  nginx-lb:
    image: nginx:alpine
    volumes:
      - ./nginx-lb.conf:/etc/nginx/nginx.conf
```

### Load Balancer Configuration
```nginx
# nginx-lb.conf
upstream ai_backends {
    server pipelines-gemini:9099 weight=3;
    server pipelines-gpt4:9099 weight=2;
    server pipelines-claude:9099 weight=1;
}

server {
    listen 80;
    location /ai/ {
        proxy_pass http://ai_backends;
    }
}
```

## üöÄ Przyk≈Çad 7: Custom Flow Modifications

### Rozszerzenie bazowego przep≈Çywu
```json
{
  "name": "Enhanced Gemini with Memory",
  "description": "Gemini z pamiƒôciƒÖ konwersacji",
  "nodes": [
    {"type": "ChatInput"},
    {"type": "ConversationMemory", "config": {"max_turns": 10}},
    {"type": "GoogleGenerativeAI", "config": {"model": "gemini-1.5-pro-latest"}},
    {"type": "ChatOutput"}
  ]
}
```

## üìä Metryki i Benchmarking

### Por√≥wnanie modeli
| Model | Avg Response Time | Cost per 1K tokens | Quality Score |
|-------|-------------------|-------------------|---------------|
| Gemini 1.5 Pro | 2.3s | $0.001 | 8.5/10 |
| GPT-4o | 1.8s | $0.005 | 9.2/10 |
| Claude-3.5 | 2.1s | $0.003 | 9.0/10 |

### Monitoring Dashboard
```python
# monitoring.py - prosty dashboard
import streamlit as st
from catalog.pipelines import *

st.title("AI Pipeline Monitoring")
st.metric("Gemini Requests", "1,234", "‚Üë12%")
st.metric("GPT-4o Requests", "856", "‚Üë5%")
st.metric("Claude Requests", "645", "‚Üë8%")
```

## üîß Troubleshooting Common Issues

### Problem: API Key nie dzia≈Ça
```bash
# Sprawd≈∫ czy klucz jest poprawnie ustawiony
docker-compose exec pipelines python3 -c "
import os
print('GEMINI_API_KEY:', 'SET' if os.getenv('GEMINI_API_KEY') else 'NOT SET')
"

# Test klucza bezpo≈õrednio
python3 -c "
import httpx
response = httpx.get('https://generativelanguage.googleapis.com/v1/models', 
                    params={'key': 'YOUR_API_KEY'})
print(response.status_code)
"
```

### Problem: Przep≈Çyw nie odpowiada
```bash
# Sprawd≈∫ logi Langflow
docker-compose logs langflow | tail -50

# Restart konkretnego serwisu
docker-compose restart langflow

# Test endpoint bezpo≈õrednio
curl -X POST http://localhost:7860/api/v1/run/gemini-chat-basic \
  -H "Content-Type: application/json" \
  -d '{"input_value": "test", "input_type": "chat"}'
```

---

## üí° Best Practices

1. **Always test in Langflow Playground first**
2. **Use environment variables for API keys**
3. **Monitor API usage and costs**
4. **Implement proper error handling**
5. **Keep backups of working flows**
6. **Use descriptive endpoint names**
7. **Document custom modifications**
8. **Test with real user scenarios**

## üîó Linki Pomocne

- [Langflow Documentation](https://docs.langflow.org/)
- [Open WebUI Pipelines Guide](https://docs.openwebui.com/pipelines/)
- [Google AI Studio](https://aistudio.google.com/)
- [OpenAI API Docs](https://platform.openai.com/docs)
- [Anthropic API Docs](https://docs.anthropic.com/)

---

*Potrzebujesz wiƒôcej przyk≈Çad√≥w? Zobacz [catalog/README.md](README.md) lub [QUICKSTART.md](QUICKSTART.md)*