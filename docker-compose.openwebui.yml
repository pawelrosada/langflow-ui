services:
  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: langflow_db
      POSTGRES_USER: langflow_user
      POSTGRES_PASSWORD: langflow_pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U langflow_user -d langflow_db"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis Cache
  redis:
    image: redis:7.2-alpine
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Langflow - Backend Python API
  langflow:
    image: langflowai/langflow:latest
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - LANGFLOW_DATABASE_URL=postgresql://langflow_user:langflow_pass@postgres:5432/langflow_db
      - LANGFLOW_CACHE_TYPE=redis
      - LANGFLOW_REDIS_URL=redis://redis:6379/0
      - LANGFLOW_AUTO_LOGIN=false
      - LANGFLOW_SUPERUSER=admin
      - LANGFLOW_SUPERUSER_PASSWORD=admin123
      - LANGFLOW_SECRET_KEY=super-secret-key-change-in-production
      - LANGFLOW_LOG_LEVEL=INFO
    volumes:
      - langflow_data:/app/langflow
    ports:
      - "7860:7860"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Pipelines - Custom Logic for Langflow Integration
  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    depends_on:
      langflow:
        condition: service_healthy
    volumes:
      - ./pipelines:/app/pipelines:ro
    environment:
      - PORT=9099
      - PIPELINES_DIR=/app/pipelines
    ports:
      - "9099:9099"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9099/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Open WebUI - Modern Chat Interface
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    depends_on:
      langflow:
        condition: service_healthy
      pipelines:
        condition: service_healthy
    environment:
      # Basic Configuration
      - WEBUI_NAME=Langflow AI Chat
      - WEBUI_URL=http://localhost:3000
      
      # Authentication
      - WEBUI_AUTH=true
      - WEBUI_SECRET_KEY=your-secret-key-change-in-production
      
      # Pipelines Integration (zamiast bezpo≈õredniego OpenAI API)
      - OPENAI_API_BASE_URL=http://pipelines:9099
      - OPENAI_API_KEY=sk-langflow-integration-key
      
      # Database
      - DATABASE_URL=postgresql://langflow_user:langflow_pass@postgres:5432/langflow_db
      
      # Features
      - ENABLE_RAG_WEB_SEARCH=true
      - ENABLE_RAG_LOCAL_WEB_FETCH=true
      - RAG_EMBEDDING_ENGINE=sentence-transformers
      
      # File uploads
      - ENABLE_IMAGE_GENERATION=false
      - ENABLE_COMMUNITY_SHARING=false
      
      # Security
      - WEBUI_SESSION_COOKIE_SAME_SITE=lax
      - WEBUI_SESSION_COOKIE_SECURE=false
      
      # Performance
      - CHUNK_SIZE=1000
      - CHUNK_OVERLAP=200
      
    volumes:
      - open_webui_data:/app/backend/data
    ports:
      - "3000:8080"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Nginx Reverse Proxy
  nginx:
    image: nginx:1.25-alpine
    depends_on:
      - open-webui
      - langflow
    volumes:
      - ./nginx.openwebui.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "80:80"
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  langflow_data:
  open_webui_data:

networks:
  default:
    name: langflow-openwebui-network
